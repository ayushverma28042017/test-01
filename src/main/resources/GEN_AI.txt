
Foundation Model > LLM
huge amnt unsupervisoed manner , unlable data
TB of data to train 
gneration next word based on trained on data set > gnerating soe new , next word
tune fondation: with saml data for your task
if not data, you can tune using prompting

Advantage:
	- performance: 
	- Productivity : 
Disadvantage:
	- Cost of compute
	- difficult to train a FM modle for small org
	- host modle and tun inference , tuff
	- Trust 

What Makes Large Language Models Expensive?
 - safeguard >sensitive , confidential and propritery data 
 -  usea case ,
    model size, : larger the model, more pram it takes, >Flan(11B) , Llama2(70 Billion param) , some good for q&A, some for summarize,basd on sued case 
	pre-training:  traning FM model from scratch >time cmpute and effort ,cost of GPT3:> 1000 GPU, 30 dyas, 4.6 million , levarage and take advanta of ,inferencing , where u giv pomt 
	infrenceing, : process generate response ,use knwledge to create , Token: 1 token = 3/4 word , cost no of token in promt and completion
	tuning, :cost factor> process of adjusting modle itself: hourly rate charges,why tune : achieve better perf from base FM, lower cost ,by deplying small model 
			- Fine tuning : extensive adaptation of model, forked version of base model
			- PEFT (parameter efficient fine tuning):: adding additonal param , addign 1k of labled data sources> prompt tuning, 
	hosting : hositng or using inference API , llm predeployed by platform provider , do as on demand , cost is no of token, 
			- hou
	deployment(cloud/SAAS/On prem)
		- cost: sAAS : using subscription fee, uding nee own GPU ,cost effective
				on-prem: if regulation : not alowed host dataon clud , purchase and maind GPU, full conrol on arch, and data
-------------------------------------------------------------------------------------------------------

Gen AI :
	Abuse Monitoring:
	Content filtering
		-running both the prompt and completion through an ensemble of classification models aimed at detecting and preventing the output of harmful content.
		- Category: Hate and fairness/sexual/voilence/self harm
	embeddings 
		-special format of data representation that machine learning models and algorithms can easily use.
		-Each embedding is a vector of floating-point numbers, such that the distance between two embeddings in the vector space is correlated with semantic similarity between two inputs in the original format.
		-Embedding models
			- Similarity embeddings
			- Text search embeddings
			- Code search embeddings
	fine-tuning:
		-Prompt Engineering:designing prompts for natural language processing models
		-Retrieval Augmented Generation (RAG) improves Large Language Model (LLM) performance by retrieving data from external sources
		- Fine-tuning retrains an existing Large Language Model using example data, resulting in a new "custom" Large
	
	GPT 4: GPT-4 Turbo with Vision is a large multimodal model (LMM) developed by OpenAI that can analyze images and provide textual responses to questions about them.
	red teaming:systematic adversarial attacks for testing security vulnerabilities
	-
	
---------------------------------------------------------------------

Hello Everyone ,
My name is Ayush, I am frm an Engg team.

Thie user of this serice cna be any cosumer or customer to know about the policy that they have with SL .
If you have fully covered now, it is good toe check if well covered under  fincacial guideline
	- Financial advisro will fincical reivew for you.
	- National database if you are well cover as per you age: 
	- safeguard >sensitive , confidential and propritery data 
	-	Cost for Llama2 on OCI 2 USD per hour per core
	-	Llama2 on OCI in future will be customer responsibility for patching/upgrading
	-	Data privacy for Gen AI Service – no customer data is stored
	- GPT-3.5 Turbo (16K)	$0.0030	$0.0040
	- security: gdpr and hipaa, use private link , own key to encrypt and decrypt during transit, not used to train base model
	- governance and audit: Azure Cloud Monitoring Servic, similar to aws cloud watch and cloud trail



We have consider all security and governace that is required , like no PII data is being used, not sensitive dta is being pass to LLM Model
data is fully secure in transit and at rest  , and have a gatwaay too to handle Ddos and MIMA attackss

Valu propostion:

lets start our demo
Scope of this POC is only Mindef and MHA and GTL , Living Care  .


Establish the Purpose:
	- Help Customer/Advisor to discover the Insuruace and suggest for new product and featurs
Agenda will be, demo for Mindef and MHA customer

Key feature:
	- based on NRIC , if it is existing customer , will diplay if info about SA and coverage
	

I appreciate your time and attention during the presentation. It was a pleasure showcasing how our [software/product/service] can [solve a specific problem/fulfill a need/bring added value] to your [company/organization/team].


If you have any further questions or require additional information, please feel free to reach out. I am more than happy to assist.

----------------------------------------------------------------------------------

1- DEAL Profess: 
	- Discovery : what is care , whoa is meant for, what all
	- Educate the audiance
	- Associate
	- Lead


GPT:
	nove 30 2022
	- 1 million in 2mnht
	- LLM: GPT3> :: typeof neural, trian on massiv amnt of tet data to under human lang.
	- lean stast patter and relationship
	- size , numbe rof param it contain: 175Billion parate in 100 layes of neural netwrk
	- input and out : token
	- number repsresnt od word token
	- mode train to predit next tokne, form give sequence.
	- make odel safer, 
	- reinforment learning from user input: create a reward modle bases on people feedback
	
---------------------------------------------------------------------

Foundation Model > LLM
huge amnt unsupervisoed manner , unlable data
TB of data to train 
gneration next word based on trained on data set > gnerating soe new , next word
tune fondation: with saml data for your task
if not data, you can tune using prompting

Advantage:
	- performance: 
	- Productivity : 
Disadvantage:
	- Cost of compute
	- difficult to train a FM modle for small org
	- host modle and tun inference , tuff
	- Trust 

What Makes Large Language Models Expensive?
 - safeguard >sensitive , confidential and propritery data 
 -  usea case ,
    model size, : larger the model, more pram it takes, >Flan(11B) , Llama2(70 Billion param) , some good for q&A, some for summarize,basd on sued case 
	pre-training:  traning FM model from scratch >time cmpute and effort ,cost of GPT3:> 1000 GPU, 30 dyas, 4.6 million , levarage and take advanta of ,inferencing , where u giv pomt 
	infrenceing, : process generate response ,use knwledge to create , Token: 1 token = 3/4 word , cost no of token in promt and completion
	tuning, :cost factor> process of adjusting modle itself: hourly rate charges,why tune : achieve better perf from base FM, lower cost ,by deplying small model 
			- Fine tuning : extensive adaptation of model, forked version of base model
			- PEFT (parameter efficient fine tuning):: adding additonal param , addign 1k of labled data sources> prompt tuning, 
	hosting : hositng or using inference API , llm predeployed by platform provider , do as on demand , cost is no of token, 
			- hour
	deployment(cloud/SAAS/On prem)
		- cost: sAAS : using subscription fee, uding nee own GPU ,cost effective
				on-prem: if regulation : not alowed host dataon clud , purchase and maind GPU, full conrol on arch, and data
				
---------------------------------------------------------------------------------------------

As an AI language model, I am not able to provide a specific tutorial on how to create an image dynamically with content in Gen AI DALL-E. However, I can explain the general process of how it works.

Gen AI DALL-E is an artificial intelligence program developed by OpenAI that generates images from textual input. To create an image dynamically, you need to provide a textual description or prompt to Gen AI DALL-E, which will then generate an image based on the input.

The process typically involves the following steps:
Define the content or idea you want to convey in the image.
Write a textual prompt or description that captures the content or idea.
Submit the prompt or description to Gen AI DALL-E.
Wait for Gen AI DALL-E to generate the image.
Review and adjust the generated image as needed.

The specific steps and tools used to create an image dynamically with content in Gen AI DALL-E may vary depending on the platform or software you are using. It's important to refer to the documentation or tutorials provided by the platform or software to ensure you are using the correct approach.

----------------------------------------------------------------------------

Multiple models, each with different capabilities and price points. Prices are per 1,000 tokens. You can think of tokens as pieces of words, where 1,000 tokens is about 750 words. This paragraph is 35 tokens.

Modle             			input          					output
GPT 4 TURBO:
gpt-4-1106-preview			$0.01 / 1K tokens				$0.03 / 1K tokens
gpt-4-1106-vision-preview	$0.01 / 1K tokens				$0.03 / 1K tokens

GPT4 :"complex instructions in natural language and solve difficult problems with accuracy"
gpt-4						$0.03 / 1K tokens				$0.06 / 1K tokens
gpt-4-32k					$0.06 / 1K tokens				$0.12 / 1K tokens

Assistants API
Assistants API and tools (retrieval, code interpreter) make it easy for developers to build AI assistants within their own applications. Each assistant incurs its own retrieval file storage fee based on the files passed to that assistant. 


TOOL						INPUT
Code interpreter		$0.03 / session (free until 12/13/2023)
Retrieval				$0.20 / GB / assistant / day (free until 01/12/2024


Fine-tuning models

Create your own custom models by fine-tuning our base models with your training data. Once you fine-tune a model, you’ll be billed only for the tokens you use in requests to that model.

Model					Training					Input usage						Output usage
gpt-3.5-turbo		$0.0080 / 1K tokens			$0.0030 / 1K tokens				$0.0060 / 1K tokens
davinci-002			$0.0060 / 1K tokens			$0.0120 / 1K tokens				$0.0120 / 1K tokens
babbage-002			$0.0004 / 1K tokens			$0.0016 / 1K tokens				$0.0016 / 1K token

Embedding models
Build advanced search, clustering, topic modeling, and classification functionality with our embeddings offering.

Model		Usage
ada v2		$0.0001 / 1K tokens

BASE MODEL
GPT base models are not optimized for instruction-following and are less capable, but they can be effective when fine-tuned for narrow tasks.

Model			Usage
davinci-002		$0.0020 / 1K tokens
babbage-002		$0.0004 / 1K tokens

Image models

Build DALL·E directly into your apps to generate and edit novel images and art. DALLE·E 3 is the highest quality model and DALL·E 2 is optimized for lower cost.

Model			Quality	Resolution						Price
DALL·E 3		Standard	1024×1024				$0.040 / image
				Standard	1024×1792, 1792×1024	$0.080 / image
DALL·E 3			HD	1024×1024					$0.080 / image
					HD	1024×1792, 1792×1024		$0.120 / image
					
Audio models

Whisper can transcribe speech into text and translate many languages into English.

Text-to-speech (TTS) can convert text into spoken audio.

Model			Usage
Whisper		$0.006 / minute (rounded to the nearest second)
TTS			$0.015 / 1K characters
TTS HD		$0.030 / 1K characters			

FAQ:
TOKEN: You can think of tokens as pieces of words used for natural language processing.
For English text, 1 token is approximately 4 characters or 0.75 words

Which model to use:
gpt-4 generally performs better on a wide range of evaluations, while gpt-3.5-turbo returns outputs with lower latency and costs much less per token

How will I know how many tokens I’ve used each month?
usage tracking dashboard.

How can I manage my speding
You can set a monthly budget in your billing settings, after which we’ll stop serving your requests. 

 ChatGPT API and ChatGPT Plus subscription are billed separately.
 Yes, we treat Playground usage the same as regular API usage.
 
 
How is pricing calculated for Completions?
 Chat completion requests are billed based on the number of input tokens sent plus the number of tokens in the output(s) returned by the API.

Your request may use up to num_tokens(input) + [max_tokens * max(n, best_of)] tokens, which will be billed at the per-engine rates outlined at the top of this page.

In the simplest case, if your prompt contains 200 tokens and you request a single 900 token completion from the gpt-3.5-turbo-1106 API, your request will use 1100 tokens and will cost [(200 * 0.001) + (900 * 0.002)] / 1000 = $0.002.

SLA:We will be publishing an SLA soon. In the meantime you can visit our Status page to monitor service availability and view historical uptime.

Azure customers can access the OpenAI API on Azure with the compliance, regional support, and enterprise-grade security that Azure offers

---------------------
AZURE GEN AI :

Properties:
Model name: gpt-35-turbo
Model version: 0301
Version update policy: Once the current version expires.
Deployment type: Standard
Content Filter: Default
Tokens per Minute Rate Limit (thousands): 120
Rate limit (Tokens per minute): 120000
Rate limit (Requests per minute): 720

	


---------------------------------------------------------------------------------------------------------------------

Why prompt:
These models can’t read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you’d like to see. The less the model has to guess at what you want

- Split complex tasks into simpler subtasks
- Give the model time to "think"
	- Asking for a "chain of thought" before an answer can help the model reason its way toward correct answers more reliably.
- Strategy: Write clear instructions

		- BAD: Who’s president?
		- GOOD:Who was the president of Mexico in 2021, and how frequently are elections held?
		-BAD: Summarize the meeting notes.
		GOOD:Summarize the meeting notes in a single paragraph. Then write a markdown list of the speakers and each of their key points. Finally, list the next steps or action items suggested by the speakers, if any.
		
- Ask the model to adopt a persona
	- System messgae is need for persona
	- EG:System: When I ask for help to write something, you will reply with a document that contains at least one joke or playful comment in every paragraph.
 - Use delimiters to clearly indicate distinct parts of the input
   - EG: Delimiters like triple quotation marks, XML tags, section titles, etc. can help demarcate sections of text to be treated differently.
   
 - SYSTEM: You will be provided with a pair of articles (delimited with XML tags) about the same topic. First summarize the arguments of each article. Then indicate which of them makes a better argument and explain why
 -USER: 
<article> insert first article here </article>

<article> insert second article here </article>

- Tactic: Specify the steps required to complete a task
		- SYSTEM >> Use the following step-by-step instructions to respond to user inputs.
					Step 1 - The user will provide you with text in triple quotes. Summarize this text in one sentence with a prefix that says "Summary: ".

					Step 2 - Translate the summary from Step 1 into Spanish, with a prefix that says "Translation: ".
					USER
						"""insert text here"""

- Provide example:
	-you intend for the model to copy a particular style of responding to user queries which is difficult to describe explicitly. This is known as "few-shot" prompting.
	- 
	
=Tactic: Specify the desired length of the output
	- USER
Summarize the text delimited by triple quotes in about 50 words.
OR
Summarize the text delimited by triple quotes in 3 bullet points.


- Strategy: Provide reference text
	- SYSTEM
			Use the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write "I could not find an answer."
	USER
		<insert articles, each delimited by triple quotes>

		Question: <insert question here>
		
: Instruct the model to answer with citations from a reference text
	-SYSTEM
		You will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: "Insufficient information." If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({"citation": …}).
	USER
		"""<insert document here>"""

Question: <insert question here>

"""insert text here"""

Use external tools
	-Use embeddings-based search to implement efficient knowledge retrieval
	- Use code execution to perform more accurate calculations or call external APIs
	- EG: SYSTEM
		You can write and execute Python code by enclosing it in triple backticks, e.g. ```code goes here```. Use this to perform calculations.
		USER
			Find all real-valued roots of the following polynomial: 3*x**5 - 5*x**4 - 3*x**3 - 7*x - 10.
    -  Give the model access to specific functions
			e.g. convert "Who are my top customers?" to get_customers(min_revenue: int, created_before: string, limit: int) and call your internal API
			
==========================================================================================================


CODEX

 -  Codex can use this data to help you write accurate query requests. 
   eg:# Table albums, columns = [AlbumId, Title, ArtistId]
		# Table artists, columns = [ArtistId, Name]
		-USER:Create a query for all albums by Adele
- Converting between languages. 
	- Python to R
		ou can get Codex to convert from one language to another by following a simple format where you list the language of the code you want to convert in a comment, followed by the code and then a comment with the language you want it translated into
	- EG:
		# Convert this from Python to R
			# Python version

			[ Python code ]

			# End

			# R version
 - Checking code for errors.
 - Writing unit tests. 
 - Explaining an SQL query
 - Creating example data. 	
	- you can ask Codex to create data like arrays of made up names, products and other variables.
	- /* Create an array of weather temperatures for San Francisco */
	- var weather = [
		{ month: 'January', high: 58, low: 48 },
		{ month: 'February', high: 61, low: 50 }]
		
		
------------------------------------------------------------------------------------------------------------------

Azure OpenAI on your data (preview)
	- enables you to run supported chat models such as GPT-35-Turbo and GPT-4 on your data without needing to train or fine-tune models.
	- valuable insights that can help you make better business decisions,
	- its ability to tailor the content of conversational AI.
	- This grounding data also helps the model avoid generating responses based on your new data
	- OpenAI's powerful GPT-35-Turbo and GPT-4 l
	- a REST API or the web-based interface in the Azure OpenAI Studio
	- Azure OpenAI on your data, together with Azure Cognitive Search, determines what data to retrieve from the designated data source based on the user input and provided conversation history
	- Azure OpenAI on your data uses an Azure Cognitive Search index to determine what data to retrieve based on user inputs and provided conversation history. 
	- The model provides the best citation titles from markdown (.md) files
	- If a document is a PDF file, the text contents are extracted as a preprocessing step (unless you're connecting your own Azure Cognitive Search index
	- Ingestion of data:
		Blobs in an Azure storage container that you provide
		Local files uploaded using the Azure OpenAI Studio
		URLs/web addresses.
		
	how data get processed after ingestion from AZURE container:
		

	1-Ingestion assets are created in Azure Cognitive Search resource and Azure storage account. Currently these assets are: indexers, indexes, data sources, a custom skill in the search resource, and a container (later called the chunks container) in the Azure storage account. You can specify the input Azure storage container using the Azure OpenAI studio, or the ingestion API.

	2- Data is read from the input container, contents are opened and chunked into small chunks with a maximum of 1024 tokens each. If vector search is enabled, the service will calculate the vector representing the embeddings on each chunk. The output of this step (called the "preprocessed" or "chunked" data) is stored in the chunks container created in the previous step.

	3- The preprocessed data is loaded from the chunks container, and indexed in the Azure Cognitive Search index.
	
	if data is fomr url: crawling component first crawl the url ans store the content is azur estorage.
	
----------------------------------------------------------------------------------------------------------------------------------------


INTRO:
	What is GEN AI:
		- AI that can crreate new content  such as text, code,mimages, music
	Benefits:
		-Improve effciency:
			- can automate many repetative task.eg: answer to customer, claim processing, generate report
		- Personalize support :
			- by understadning customer need and preference
		- Improve customer satisfaction
		   - 
	AI Is Transforming Customer Service and Support
	
	- Generative AI, powered by advanced algorithms such as ChatGPT developed by OpenAI, 
	- technology is changing the way businesses operate by enabling more efficient and impactful communication with customers, generating new insights and workflows, and automating repetitive tasks that 	previously  consumed valuable time and resources.
	

Use cases for generative AI that help Customer Service and Support organizations to enhance customer experience, optimize operations, and reduce costs. 
 - Why gen AI is well suited:
	- can generate human-like responses quickly and accurately to a wide range of customer inquiries, including complex and nuanced questions. 
	- technology can provide 24/7 customer service and support, reducing the need for human agents and increasing efficiency.
	
	- customer data to identify insights from customer sentiment, behavior, preferences, patterns, and trends, which can improve customer experience
	- identify potential issues before they become significant problems.
	- handling a large volume of inquiries without the need for additional staff
	- 
  Generative AI Use Cases
	- Customer Self-service: freeing support staff to focus on more complex issues.
	- Chatbots and virtual assistants:Handle routine queries and tasks, provide 24/7 support to customers
	- Sentiment analysis:prevent escalations, improve customer satisfaction, and reduce churn
	- Predictive support: allowing support engineers to proactively reach out to customers and resolve issues
	- Personalized support: Analyze customer data and generate tailored content, such as product recommendations
	- Interactive tutorials:help them understand how to use products or services,
	- Automated ticket classification and routing: Analyze incoming customer requests, identify the intent of the request, and route it to the appropriate department or agent for resolution,
	-personalize recommenndation:such as peoduct risk management advise, prevention tips,
	
  Six Steps to Successfully Deploy AI-based Solutions
	- Define Your Use case: 
		- Identify your organization's specific challenges and pain points, such as high ticket volume, long response times, or case escalations. 
	- Choose an AI-based Platform: 
		- platform's features, ease of integration, cost, and customer support. 
	- Train and Fine-Tune the Model: requires providing relevant data, such as customer tickets and responses, to the platform
	- Integrate the Model Into Your Environment 
	- Monitor and Refine Performance: 

Exploring Options for Implementing Generative AI
	-Utilizing Internal Data Scientists for AI Solutions
	- Partner with Incumbent Vendors for AI Solutions
	
---------------------------------------------------------------------------------------------------------------------------------------------------------------

Generative AI refers to a subset of artificial intelligence that focuses on creating new content or data rather than simply analysing or interpreting existing information.
